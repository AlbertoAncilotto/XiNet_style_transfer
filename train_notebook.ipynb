{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de142d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "\n",
    "import clip\n",
    "import vgg\n",
    "import transformer\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb42547f",
   "metadata": {},
   "source": [
    "# Option 1: style image\n",
    "![style image](images\\\\starry_night_clone_small_256.jpg \"style\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1cfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2: Config\n",
    "DATASET_PATH = '../../datasets/coco2017'\n",
    "STYLE_IMAGE_PATH = 'images/starry_night_clone_small_256.jpg'\n",
    "SAVE_MODEL_PATH = 'checkpoints/starry256_a100/model/'\n",
    "SAVE_IMAGE_PATH = 'checkpoints/starry256_a100/images/'\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "PRETRAIN_BATCHES = 500\n",
    "TRAIN_IMAGE_SIZE = 196\n",
    "STYLE_SCALES = 1\n",
    "STYLE_DOWNSAMPLE = 0\n",
    "STYLE_WEIGHT = 10\n",
    "CONTENT_WEIGHT = 1e-4\n",
    "VARIATION_WEIGHT = 1e-3\n",
    "CLIP_WEIGHT = 2\n",
    "COLOR_WEIGHT = 2e2\n",
    "MSE_WEIGHT = 1e-3\n",
    "ALPHA = 1.00\n",
    "BN_INSTEAD_OF_IN = True\n",
    "BATCH_SIZE = 4\n",
    "SEED = 11\n",
    "ADAM_LR = 1e-3\n",
    "CONTENT_LAYER = 'relu4_4'\n",
    "CLIP_IMG_SZ = 224\n",
    "STYLE_WEIGHTS_DICT = {'relu1_2': 1, 'relu2_2': 1, 'relu3_4': 1, 'relu4_2': 1, 'relu4_4': 1, 'relu5_4': 1}\n",
    "SAVE_MODEL_EVERY = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32786395",
   "metadata": {},
   "source": [
    "# Option 2: style image\n",
    "![style image](images\\\\spaghetti.jpg \"style\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2: Config\n",
    "DATASET_PATH = '../../datasets/coco2017'\n",
    "STYLE_IMAGE_PATH = 'images/spaghetti.jpg'\n",
    "SAVE_MODEL_PATH = 'models/spaghetti256_a75/model/'\n",
    "SAVE_IMAGE_PATH = 'models/spaghetti256_a75/images/'\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "PRETRAIN_BATCHES = 500\n",
    "TRAIN_IMAGE_SIZE = 196\n",
    "STYLE_SCALES = 1\n",
    "STYLE_DOWNSAMPLE = 1\n",
    "STYLE_WEIGHT = 3\n",
    "CONTENT_WEIGHT = 3e-4\n",
    "VARIATION_WEIGHT = 1e-3\n",
    "CLIP_WEIGHT = 2\n",
    "COLOR_WEIGHT = 1e1\n",
    "MSE_WEIGHT = 1e-4\n",
    "ALPHA = 0.75\n",
    "BN_INSTEAD_OF_IN = True\n",
    "BATCH_SIZE = 4\n",
    "SEED = 11\n",
    "ADAM_LR = 1e-3\n",
    "CONTENT_LAYER = 'relu4_4'\n",
    "CLIP_IMG_SZ = 224\n",
    "STYLE_WEIGHTS_DICT = {'relu1_2': 1, 'relu2_2': 1, 'relu3_4': 1, 'relu4_2': 1, 'relu4_4': 1, 'relu5_4': 1}\n",
    "SAVE_MODEL_EVERY = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f04730",
   "metadata": {},
   "source": [
    "# Option 3: style image\n",
    "![style image](images\\\\picasso_muse_256.jpg \"style\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09902993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2: Config\n",
    "DATASET_PATH = '../../datasets/coco2017'\n",
    "STYLE_IMAGE_PATH = 'images/picasso_muse_256.jpg'\n",
    "SAVE_MODEL_PATH = 'checkpoints/muse_bn_a75/model/'\n",
    "SAVE_IMAGE_PATH = 'checkpoints/muse_bn_a75/images/'\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "PRETRAIN_BATCHES = 500 #2000\n",
    "TRAIN_IMAGE_SIZE = 256\n",
    "STYLE_SCALES = 1\n",
    "STYLE_DOWNSAMPLE = 0\n",
    "STYLE_WEIGHT = 5\n",
    "CONTENT_WEIGHT = 1e-4\n",
    "VARIATION_WEIGHT = 1e-3\n",
    "CLIP_WEIGHT = 2\n",
    "COLOR_WEIGHT = 2e2\n",
    "MSE_WEIGHT = 1e-3\n",
    "ALPHA = 0.75\n",
    "BN_INSTEAD_OF_IN = True\n",
    "BATCH_SIZE = 4\n",
    "SEED = 11\n",
    "ADAM_LR = 1e-3\n",
    "CONTENT_LAYER = 'relu4_4'\n",
    "CLIP_IMG_SZ = 224\n",
    "STYLE_WEIGHTS_DICT = {'relu1_2': 1, 'relu2_2': 1, 'relu3_4': 1, 'relu4_2': 1, 'relu4_4': 1, 'relu5_4': 1}\n",
    "SAVE_MODEL_EVERY = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3: Setup\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(TRAIN_IMAGE_SIZE),\n",
    "    transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "\n",
    "transform_clip = transforms.Compose([\n",
    "    transforms.Resize(CLIP_IMG_SZ),\n",
    "    transforms.Lambda(lambda x: x.div(255.0)),\n",
    "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "imagenet_neg_mean = torch.tensor([-103.939, -116.779, -123.68], dtype=torch.float32).reshape(1,3,1,1).to(device)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "os.makedirs(SAVE_MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(SAVE_IMAGE_PATH, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 4: Load models\n",
    "TransformerNetwork = transformer.TransformerNetworkXiNet(alpha=ALPHA,  beta=1.0, gamma=4, lite=False, num_pool=STYLE_SCALES-1, bn_instead_of_in=BN_INSTEAD_OF_IN).to(device)\n",
    "VGG = vgg.VGG19().to(device)\n",
    "CLIP_NET, _ = clip.load('RN50', device)\n",
    "CLIP_NET.eval().float()\n",
    "CLIP_NET.requires_grad_(False)\n",
    "\n",
    "intermediate_tensor = None\n",
    "def hook_fn(module, input, output):\n",
    "    global intermediate_tensor\n",
    "    intermediate_tensor = output.clone()\n",
    "\n",
    "CLIP_NET.visual.layer4.register_forward_hook(hook_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a19a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 5: Style image\n",
    "style_grams = []\n",
    "\n",
    "for style_id in range(STYLE_SCALES):\n",
    "    style_tensor = utils.itot(utils.load_image(STYLE_IMAGE_PATH), scale=True).to(device)\n",
    "    style_tensor = style_tensor.add(imagenet_neg_mean)\n",
    "    style_tensor = nn.functional.avg_pool2d(style_tensor, 2**(style_id + STYLE_DOWNSAMPLE))\n",
    "    B, C, H, W = style_tensor.shape\n",
    "    features = VGG(style_tensor.expand([BATCH_SIZE, C, H, W]))\n",
    "    style_gram = {k: utils.scale(utils.gram(v)) for k, v in features.items()}\n",
    "    style_grams.append(style_gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae308e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 6: Loss computation\n",
    "def total_variation_loss(img):\n",
    "    bs_img, c_img, h_img, w_img = img.size()\n",
    "    tv_h = torch.pow(img[:,:,1:,:]-img[:,:,:-1,:], 2).sum()\n",
    "    tv_w = torch.pow(img[:,:,:,1:]-img[:,:,:,:-1], 2).sum()\n",
    "    return (tv_h+tv_w)/(bs_img*c_img*h_img*w_img)\n",
    "\n",
    "def color_loss(generated, original):\n",
    "    assert generated.shape == original.shape, \"Content and style images must have the same shape.\"\n",
    "\n",
    "    original_ycbcr = utils.rgb2ycbcr(torch.clamp(original/255.0, 0, 1))\n",
    "    generated_ycbcr = utils.rgb2ycbcr(torch.clamp(generated/255.0, 0, 1))\n",
    "\n",
    "    original_cb, original_cr = original_ycbcr[:, 1, :, :], original_ycbcr[:, 2, :, :]\n",
    "    generated_cb, generated_cr = generated_ycbcr[:, 1, :, :], generated_ycbcr[:, 2, :, :]\n",
    "\n",
    "    mse_cb = F.mse_loss(original_cb, generated_cb)\n",
    "    mse_cr = F.mse_loss(original_cr, generated_cr)\n",
    "\n",
    "    total_color_loss = mse_cb + mse_cr\n",
    "\n",
    "    return total_color_loss\n",
    "\n",
    "def clip_loss(content_batch, generated_batch, clip_net, transform_clip):\n",
    "    content_clip = transform_clip(content_batch)\n",
    "    gen_clip = transform_clip(generated_batch)\n",
    "    clip_feat_c = clip_net.encode_image(content_clip)\n",
    "    clip_feat_g = clip_net.encode_image(gen_clip)\n",
    "    return (1 - torch.cosine_similarity(clip_feat_c, clip_feat_g).mean())\n",
    "\n",
    "def style_loss(generated_batches, style_grams, STYLE_WEIGHT, STYLE_WEIGHTS_DICT):\n",
    "    MSELoss = nn.MSELoss()\n",
    "    total_style_loss = 0\n",
    "    for scale_id, generated in enumerate(generated_batches):\n",
    "        features = VGG(generated.add(imagenet_neg_mean))\n",
    "        for layer, weight in STYLE_WEIGHTS_DICT.items():\n",
    "            target_gram = utils.scale(style_grams[scale_id][layer])\n",
    "            generated_gram = utils.scale(utils.gram(features[layer]))\n",
    "            total_style_loss += STYLE_WEIGHT * weight * MSELoss(generated_gram, target_gram)\n",
    "    return total_style_loss\n",
    "\n",
    "def content_loss(content_batch, generated_batch, vgg, layer):\n",
    "    MSELoss = nn.MSELoss()\n",
    "    content_features = vgg(content_batch.add(imagenet_neg_mean))\n",
    "    generated_features = vgg(generated_batch.add(imagenet_neg_mean))\n",
    "    return MSELoss(generated_features[layer], content_features[layer])\n",
    "\n",
    "def compute_loss(content_batch, generated_batches, vgg, clip_net, transform_clip, pretrain=False):\n",
    "    MSELoss = nn.MSELoss()\n",
    "    losses = {}\n",
    "\n",
    "    losses['content'] = CONTENT_WEIGHT * content_loss(content_batch, generated_batches[0], vgg, CONTENT_LAYER)\n",
    "    losses['mse'] = MSE_WEIGHT * MSELoss(generated_batches[0], content_batch)\n",
    "\n",
    "    if pretrain:\n",
    "        total_loss = losses['content'] + losses['mse']\n",
    "        return (total_loss, losses)\n",
    "\n",
    "    losses['variation'] = VARIATION_WEIGHT * total_variation_loss(generated_batches[0])\n",
    "    losses['clip'] = CLIP_WEIGHT * clip_loss(content_batch, generated_batches[0], clip_net, transform_clip)\n",
    "    losses['color'] = COLOR_WEIGHT * color_loss(generated_batches[0], content_batch)\n",
    "    losses['style'] = style_loss(generated_batches, style_grams, STYLE_WEIGHT, STYLE_WEIGHTS_DICT)\n",
    "\n",
    "    total_loss = sum(losses.values())\n",
    "    return (total_loss, losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fbde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 7: Training loop\n",
    "optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n",
    "batch_count = 0\n",
    "losses_sum = {'content': 0, 'style': 0, 'variation': 0, 'clip': 0, 'color': 0, 'mse': 0}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    for content_batch, _ in tqdm(train_loader):\n",
    "        batch_count += 1\n",
    "        content_batch = content_batch[:, [2, 1, 0]].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        generated_batches = TransformerNetwork.forward_multiscale(content_batch)\n",
    "\n",
    "        total_loss, loss_components = compute_loss(\n",
    "            content_batch, generated_batches, VGG, CLIP_NET, transform_clip, pretrain=(batch_count < PRETRAIN_BATCHES)\n",
    "        )\n",
    "\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(TransformerNetwork.parameters(), 2)\n",
    "        optimizer.step()\n",
    "\n",
    "        for k in losses_sum.keys():\n",
    "            if k in loss_components:\n",
    "                losses_sum[k] += loss_components[k].item()\n",
    "\n",
    "        # Logging and saving every SAVE_MODEL_EVERY iterations\n",
    "        if ((batch_count - 1) % SAVE_MODEL_EVERY == 0) or (batch_count == NUM_EPOCHS * len(train_loader)):\n",
    "            utils.log_losses(batch_count, losses_sum)\n",
    "            utils.save_outputs(batch_count, content_batch, generated_batches[0], TransformerNetwork, SAVE_MODEL_PATH, SAVE_IMAGE_PATH)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
